{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choice of Activation Function for Output Layer\n",
    "\n",
    "We need to choose the softmax function for the output layer with 4 output neurons in our case. This is because we are dealing with a multi-class classification problem with non-overlapping categories. A function which seems reasonable to use is the sigmoid, but this is not appropriate here. Sigmoid is best suited for binary classification or for multi-class classification where the categories are not mutually exclusive (for example, does this picture contain a dog or a cat, or both?). \n",
    "\n",
    "Source: https://stats.stackexchange.com/questions/218542/which-activation-function-for-output-layer\n",
    "\n",
    "> Regression: linear (because values are unbounded)\n",
    "> Classification: softmax (simple sigmoid works too but softmax works better)\n",
    "> Use simple sigmoid only if your output admits multiple \"true\" answers, for instance, a network that checks for the presence of various objects > in an image. In other words, the output is not a probability distribution (does not need to sum to 1).\n",
    "\n",
    "Source: [Introduction to Statistical Learning](https://www.statlearning.com/)\n",
    "\n",
    "Checking page 140, equation 4.11, we see that the sigmoid is a special case of the softmax function. In particular, the sigmoid function [1] is defined by $$\\sigma(z) = \\frac{1}{1+e^{-z}}$$ while the softmax function [1, 2] is the vector defined as $$\\textrm{softmax}(z)_i = \\frac{e^{z_i}}{1 + \\sum_{j=1}^{K} e^{-z_j}}.$$\n",
    "\n",
    "Hence, if $J=2$, we get that the softmax function gives the vector $(\\sigma(z_1), 1-\\sigma(z_1))$. This is equivalent to using the sigmoid function.\n",
    "\n",
    "\n",
    "# References \n",
    "\n",
    "[1] https://www.pinecone.io/learn/softmax-activation/\n",
    "\n",
    "[2] https://www.singlestore.com/blog/a-guide-to-softmax-activation-function/\n",
    "\n",
    "# Appendix\n",
    "\n",
    "Here's a simple code to convince you that this is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Sigmoid Model Accuracy: 0.98\n",
      "\n",
      "Sigmoid Model Predictions (First 5 Examples):\n",
      "Prediction: 1, True Label: 1, Probability: 0.7786\n",
      "Prediction: 1, True Label: 1, Probability: 0.9997\n",
      "Prediction: 1, True Label: 0, Probability: 0.6135\n",
      "Prediction: 0, True Label: 0, Probability: 0.4418\n",
      "Prediction: 0, True Label: 0, Probability: 0.0033\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "Softmax Model Accuracy: 0.99\n",
      "\n",
      "Softmax Model Predictions (First 5 Examples):\n",
      "Prediction: 1, True Label: 1, Probabilities: [0.15318818 0.84681183]\n",
      "Prediction: 1, True Label: 1, Probabilities: [2.7668298e-06 9.9999726e-01]\n",
      "Prediction: 0, True Label: 0, Probabilities: [0.54315895 0.45684102]\n",
      "Prediction: 0, True Label: 0, Probabilities: [0.72805595 0.27194405]\n",
      "Prediction: 0, True Label: 0, Probabilities: [0.99405706 0.00594293]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Generate a custom dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "X = np.random.rand(n_samples, 3)  # 100 samples, 3 features (x1, x2, x3)\n",
    "y = (X[:, 0] + X[:, 1]**2 + X[:, 2]**3 > 1).astype(int)  # y = 1 if x1 + x2^2 + x3^3 > 1, else 0\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build a simple neural network model\n",
    "def create_model(output_activation):\n",
    "    model = Sequential([\n",
    "        tf.keras.Input(shape=(X_train.shape[1],)),  # Define input shape here to avoid the warning\n",
    "        Dense(10, activation='relu'),\n",
    "        Dense(5, activation='relu'),\n",
    "        Dense(1, activation=output_activation)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train model with sigmoid activation\n",
    "model_sigmoid = create_model(output_activation='sigmoid')\n",
    "model_sigmoid.fit(X_train, y_train, epochs=50, verbose=0)\n",
    "sigmoid_probs = model_sigmoid.predict(X_test)  # Probabilities from sigmoid\n",
    "sigmoid_preds = (sigmoid_probs > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "sigmoid_accuracy = np.mean(sigmoid_preds.flatten() == y_test)\n",
    "print(f\"Sigmoid Model Accuracy: {sigmoid_accuracy:.2f}\")\n",
    "\n",
    "# Print sigmoid predictions, true labels, and probabilities for review\n",
    "print(\"\\nSigmoid Model Predictions (First 5 Examples):\")\n",
    "for i in range(5):\n",
    "    print(f\"Prediction: {sigmoid_preds[i][0]}, True Label: {y_test[i]}, Probability: {sigmoid_probs[i][0]:.4f}\")\n",
    "\n",
    "# Train model with softmax activation\n",
    "# Note: For softmax, we need 2 output neurons to represent the two classes\n",
    "def create_model_softmax():\n",
    "    model = Sequential([\n",
    "        tf.keras.Input(shape=(X_train.shape[1],)),  # Define input shape here to avoid the warning\n",
    "        Dense(10, activation='relu'),\n",
    "        Dense(5, activation='relu'),\n",
    "        Dense(2, activation='softmax')  # Two neurons for binary classes 0 and 1\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model_softmax = create_model_softmax()\n",
    "model_softmax.fit(X_train, y_train, epochs=50, verbose=0)\n",
    "softmax_probs = model_softmax.predict(X_test)  # Probabilities from softmax\n",
    "softmax_preds = np.argmax(softmax_probs, axis=1)  # Convert probabilities to class predictions\n",
    "softmax_accuracy = np.mean(softmax_preds == y_test)\n",
    "print(f\"\\nSoftmax Model Accuracy: {softmax_accuracy:.2f}\")\n",
    "\n",
    "# Print softmax predictions, true labels, and probabilities for review\n",
    "print(\"\\nSoftmax Model Predictions (First 5 Examples):\")\n",
    "for i in range(5):\n",
    "    print(f\"Prediction: {softmax_preds[i]}, True Label: {y_test[i]}, Probabilities: {softmax_probs[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differences stem from the loss function. You need to use the sparse categorical cross-entropy for softmax. There are also 2 output neurons for the NN trained with softmax output layer, so the backpropagation (fitting stage) will be slightly different. Softmax outputs a probability distribution over two classes, while sigmoid outputs a single probability value for one class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Parallelism\n",
    "\n",
    "> \"Access to compute is not enough to train good neural networks. Researchers need to use a completely different paradigm where data and model weights are distributed across different devices — and sometimes even different computers.\" [4]  \n",
    "\n",
    "GPUs (graphics processing units) have revolutionized high-performance computing, enabling tasks that were previously computationally prohibitive. Originally designed for gaming, GPUs were optimized for rendering complex graphics efficiently. Today, their parallel processing capabilities make them indispensable in fields such as scientific research, video processing, and machine learning [2]. This technological importance is reflected in Nvidia’s milestone of achieving a $2 trillion market capitalization, underscoring the role of GPUs in modern computing [1].\n",
    "\n",
    "Central to the power of GPUs is parallelism, the ability to perform many calculations simultaneously. This feature allows GPUs to handle massive computational workloads with remarkable efficiency, particularly in domains requiring large-scale data processing or repetitive operations.\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Embed GIF</title>\n",
    "    <style>\n",
    "        .gif-container {\n",
    "            text-align: center;\n",
    "        }\n",
    "        .gif-caption {\n",
    "            font-size: 16px;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"gif-container\">\n",
    "        <img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*4K4CPA7-Y1HYPMQKFoCi5Q.gif\" alt=\"Sample GIF\">\n",
    "        <p class=\"gif-caption\">Figure from Medium [5], accessed 17/11/2024, showing the power of parallel processing with GPUs. The leftmost picture indicates serial processing.</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "## GPU vs CPU\n",
    "\n",
    "A CPU core is a single processing unit within the CPU that executes instructions sequentially [6]. CPUs typically have few cores (4 to 24 [13]), while GPUs feature many more. For example, the Nvidia RTX 3090 features 10,496 cores [7]. GPU cores are smaller, less powerful, and have less memory than CPU cores but excel at handling repetitive tasks in parallel [8].\n",
    "\n",
    "CPUs are optimized for sequential task execution and can quickly switch between varied instructions, making them versatile for general-purpose tasks. GPUs, in contrast, are designed for parallelism. We say that they use their thousands of *cores* to process many **threads** of a task simultaneously. This design makes GPUs especially effective for data-heavy operations, such as rendering graphics or training machine learning models.\n",
    "\n",
    "<html>\n",
    "<head>\n",
    "    <title>Image with Caption</title>\n",
    "</head>\n",
    "<body>\n",
    "    <div style=\"text-align: center;\">\n",
    "        <img src=\"https://www.cgdirector.com/wp-content/uploads/media/2021/06/Differences-between-GPU-and-CPU-cores.jpg\" \n",
    "             alt=\"Differences between GPU and CPU cores\" \n",
    "             style=\"width: 90%; max-width: 500px; height: auto;\">\n",
    "        <p>Differences between GPU and CPU cores, taken from [8], accessed 17/11/2024.</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "## Data Parallelism: Divide and Conquer\n",
    "GPU parallelism is based on the SIMD (Single Instruction, Multiple Data) model [14], a technique where the same operation is applied to multiple data elements at the same time. This allows GPUs to execute the same instruction across multiple data points simultaneously, a concept known as **data parallelism**. This allows GPUs to process vast amounts of data in parallel, which dramatically speeds up computations compared to traditional CPU-based approaches. \n",
    "\n",
    "This is particularly effective for applications that process large datasets and where the same operation needs to be applied repeatedly to each data element. Examples include scientific simulations, image and video processing, and machine learning tasks such as training deep neural networks, which involve applying the same mathematical operations across large sets of data (e.g., matrix multiplications, convolutions).\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Image with Caption</title>\n",
    "</head>\n",
    "<body>\n",
    "    <div style=\"text-align: center;\">\n",
    "        <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*a9zrtdqLQbBATH0EBmheFQ.png\" \n",
    "             alt=\"Sample Image\" \n",
    "             style=\"width: 50%; max-width: 400px; height: auto;\">\n",
    "        <p>Image showing data parallelism: taken from [10], accessed 17/11/2024.</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "In contrast, traditional CPU parallelism typically involves executing different instructions on different tasks or data, which makes CPUs well-suited for tasks that require more complex decision-making or varied instructions. However, when it comes to applications that involve repeated identical computations on large datasets, the GPU’s design proves far more efficient. Further details, including how data parallelism helps with efficient weight updates with gradient accumulation can be seen in [4, 10].\n",
    "\n",
    "Another way in which GPUs can employ parallelism is *model parallelism*.\n",
    "\n",
    "# Model Parallelism: Sharing the Load\n",
    "\n",
    "Model parallelism is a parallel computing technique where different parts or sections of a neural network model run on different devices or nodes. This approach is particularly beneficial when dealing with very large models that don’t fit entirely within the memory of a single GPU. Instead of dividing the data, as in data parallelism, model parallelism divides the model itself. This ensures that even models with a vast number of parameters can be trained by leveraging the combined memory of multiple GPUs.\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Image with Caption</title>\n",
    "</head>\n",
    "<body>\n",
    "    <div style=\"text-align: center;\">\n",
    "        <img src=\"https://images.ctfassets.net/xjan103pcp94/3dXMEU8MDlwyreIB7bFMwI/9c755e4a7c5aa9f314c49cbeac21ab4c/blog-what-is-distributed-training-data-vs-model-parallelism.png\" \n",
    "             alt=\"Distributed Training: Data vs Model Parallelism\" \n",
    "             style=\"width: 60%; max-width: 500px; height: auto;\">\n",
    "        <p>Distributed Training: Data vs Model Parallelism. Taken from [11], accessed 17/11/2024.</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "The biggest manufacturers of GPUs, NVIDIA, AMD and Intel, have different ways they structure their GPUs to allow effective parallelism. Efficient execution on GPUs also requires careful consideration of memory management. Optimizations such as memory coalescing, where memory accesses are grouped together to reduce latency, and kernel optimization, where the computational workload is streamlined, are essential to maximizing performance. These optimizations help mitigate the challenges associated with memory bandwidth limitations and synchronization overhead, which are common bottlenecks in parallel computing environments.\n",
    "\n",
    "# Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The advantages of GPU parallelism are most evident in tasks that require processing large volumes of data with high computational demands. In our case, training a CNN is highly computationally intensive. GPUs dramatically accelerate these processes, reducing the time required for training models from days to hours. We have certainly not covered all there is to parallelism. A more technical tutorial, with details on how to minimise idle time of the GPU and ensure high and efficient GPU usage, is given in [12]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## References  \n",
    "[1]: \"Nvidia hits $2 trillion market cap as AI chips fuel growth.\" *The Verge*, February 23, 2024. [Link](https://www.theverge.com/2024/2/23/24080975/nvidia-ai-chips-h100-h200-market-capitalization).  \n",
    "[2]: \"Harnessing Parallelism: How GPUs Revolutionize Computing.\" *Medium*, 2024. [Link](https://medium.com/accredian/harnessing-parallelism-how-gpus-revolutionize-computing-597f3479d955).  \n",
    "[3]: \"Why GPU Parallel Computing Matters to CEOs.\" *Accredian*, 2024.  \n",
    "[4] https://www.blopig.com/blog/2023/10/understanding-gpu-parallelization-in-deep-learning/ \n",
    "\n",
    "[5] https://medium.com/accredian/harnessing-parallelism-how-gpus-revolutionize-computing-597f3479d955\n",
    "\n",
    "[6] https://www.lenovo.com/gb/en/glossary/cpu-core/?srsltid=AfmBOopBBKLDMrm9anJmgOaPCRfFc5oPJlLyXZOYzDkLKmlOxAqI5T8i\n",
    "\n",
    "[7] https://www.youtube.com/watch?v=r9IqwpMR9TE\n",
    "\n",
    "[8] https://www.cgdirector.com/wp-content/uploads/media/2021/06/Differences-between-GPU-and-CPU-cores.jpg\n",
    "\n",
    "[9] https://aws.amazon.com/compare/the-difference-between-gpus-cpus/#:~:text=GPUs%20excel%20in%20parallel%20processing,them%20through%20at%20high%20speed\n",
    "\n",
    "[10] https://kozodoi.me/blog/20210219/gradient-accumulation\n",
    "\n",
    "[11] https://images.ctfassets.net/xjan103pcp94/3dXMEU8MDlwyreIB7bFMwI/9c755e4a7c5aa9f314c49cbeac21ab4c/blog-what-is-distributed-training-data-vs-model-parallelism.png\n",
    "\n",
    "[12] https://enccs.github.io/gpu-programming/4-gpu-concepts/\n",
    "\n",
    "[13] https://www.intel.com/content/www/us/en/products/details/processors/core/i9/products.html\n",
    "\n",
    "[14] https://stackoverflow.com/questions/77445710/can-modern-cpus-run-in-simt-mode-like-a-gpu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

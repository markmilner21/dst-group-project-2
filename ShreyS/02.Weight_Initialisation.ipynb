{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1467996-168b-433d-b49e-2655d6730b1f",
   "metadata": {},
   "source": [
    "# __Weight Initialisation Techniques for Deep Neural Networks__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3549759d-fe08-42a7-86f1-d6ccb9cc4baa",
   "metadata": {},
   "source": [
    "Building and training models for neural networks requires some prerequisites to ensure stable and optimal results when analyzing over a specific amount of data. To ensure high accuracy, one should familiarise themselves with the practice of weight initialisation. While this is just a simple code in python, the theory that goes behind it is very interesting, since if the weights are not initialised porperly, it may give rise to the Vanishing or Exploding Gradient Problem [[1]](https://www.geeksforgeeks.org/vanishing-and-exploding-gradients-problems-in-deep-learning/). Some of the notations are already explained in the Tanh investigation notebook. The image below represents an interconnected feed-forward neural netwrok."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165e3f73-f8de-497c-93f7-eb61c574e062",
   "metadata": {},
   "source": [
    "![NN](NN.png)   \n",
    "\n",
    "Each unit of the network performs a non-linear transformation (activation function) of a weighted sum of the outputs $x_{i}$ of the units of the previous level to generate its own output $y$:   \n",
    "$$ y = F \\left( w_{0} + \\sum_{i}{w_{i} x_{i}} \\right) $$   \n",
    "\n",
    "The bias is considered an additional unit with an output equal to 1 and the weight $w_{0}$, and has the function of $y$-intercept, without which the model generated by the network is forced to pass from the origin in the space of the problem, that is the point $(\\mathbf{x}=0,\\mathbf{y}=0)$. The bias adds flexibility and allows modeling datasets for which this condition is not met. [[2]](https://www.baeldung.com/cs/ml-neural-network-weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35caed02-d507-4968-8472-300b287d30b9",
   "metadata": {},
   "source": [
    "## __Weight Initialisation techniques__   \n",
    "These are the different ways of initialising weights in a neural network, which could be selecting a constant number for all the weights in the network or randomnising the weights in a specific range. The \"best\" practice is to generate a random set of weights with an initial bias of 0, which corresponds to \"breaking the symmetry\" so that each neuron performs different computations. Why breaking the symmetry? This particular condition severely penalizes training which leads to bad model performance and prediction on unseen data. [[2]](https://www.baeldung.com/cs/ml-neural-network-weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d31fd6-dbe6-4f65-a9a2-de62409920bc",
   "metadata": {},
   "source": [
    "#### __1. Zero Initialisation__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc98ff75-446b-41ed-b5f5-25d1a3742527",
   "metadata": {},
   "source": [
    "As the name suggests, all the weights are assigned zero and this kind of initialization is highly ineffective as neurons learn the same feature during each iteration. Rather, during any kind of constant initialization, the same issue happens to occur. Thus, constant initializations are not preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca34dbc-c4b6-47fb-ab66-bdabcc98ebc0",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce24001-ebd0-4038-8eee-4d696e3f88f9",
   "metadata": {},
   "source": [
    "#### __2. Random Initialization__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f2a718-5898-4ac9-b78d-236ec0bf73b3",
   "metadata": {},
   "source": [
    "To overcome the problems caused by zero initialisation, this method assigns random values to neuron paths, other than zero. However in this case, one of the major drawbacks is the causation of Vanishing or Exploding gradients due to randomness. They can be categorized into Random Normal and Random Uniform. As the name suggests, weights are initialised from the values in a normal and unifrom dsitribution respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ce8f34-8fc4-4b6e-b469-c0e6267a452f",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a896d0-f088-450c-a2da-e985edc36bd6",
   "metadata": {},
   "source": [
    "#### __3. Xavier/Glorot Initialization__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dd602c-46e0-4120-91de-a391409ae51d",
   "metadata": {},
   "source": [
    "Xavier Glorot in 2010 developed this technique in his paper \"Understanding the difficulty of training deep feedforward neural networks\" [[3]](http://proceedings.mlr.press/v9/glorot10a.html) based on the idea that allows initial weights to be set in a such way that activation and gradients can flow freely and effectively in both forward and backpropagation (previous 2 methods cause problems especially during backpropagation). With each passing layer, Xavier initialisation maintains the variance to some extent taking full advantage of the activation function through 2 stratergies :\n",
    "<br>\n",
    "a) Uniform Xavier Initialisation - drawing each weight from a uniform dsitribution in the range $ [-x,x] $, where :\n",
    "$$ x = \\sqrt{\\frac{6}{inputs+outputs}} $$ [[4]](https://365datascience.com/tutorials/machine-learning-tutorials/what-is-xavier-initialization/#h_24242636975541686829817569)  \n",
    "\n",
    "b) Normal Xavier Initialisation - drawing each weight from a normal distribution with mean 0 and standard deviation :\n",
    "$$ \\sigma = \\sqrt{\\frac{2}{inputs+outputs}} $$ [[4]](https://365datascience.com/tutorials/machine-learning-tutorials/what-is-xavier-initialization/#h_24242636975541686829817569)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89a76f6-da02-4cbe-bed2-ed51565db907",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb155a7a-a950-4de7-9c9e-4b0bc1be2492",
   "metadata": {},
   "source": [
    "Higher number of outputs implies greater need to spread the weights since the output layer consists of the activation function in question. [[4]](https://365datascience.com/tutorials/machine-learning-tutorials/what-is-xavier-initialization/#h_24242636975541686829817569)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53537fcd-205d-4088-a841-3feac78beb8c",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a777be04-e81c-4f22-89b8-a1472e0df96c",
   "metadata": {},
   "source": [
    "![Glorot1](Glorot1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240c9a8d-5593-42e6-bb6d-4e85206b901d",
   "metadata": {},
   "source": [
    "During backpropagation, optimization occurs in the backward direction, hence weights need to be initalised even in the input layers so as obtain optimal results. [[4]](https://365datascience.com/tutorials/machine-learning-tutorials/what-is-xavier-initialization/#h_24242636975541686829817569)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14027539-46bc-4ba3-83b0-76c5c9151602",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905e3fc0-4597-4a5c-8e10-bedeeb9034ed",
   "metadata": {},
   "source": [
    "![Glorot2](Glorot2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0f83d7-bcfe-4143-b66c-d1ba6d5892e7",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c81fa95-775e-4c0b-8201-71bac3c74028",
   "metadata": {},
   "source": [
    "#### __4. Kaiming or He Initialisation__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecb29ef-0904-47ce-89a3-323920fe9613",
   "metadata": {},
   "source": [
    "Developed in the paper \"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification‚Äù by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun [[5]](https://openaccess.thecvf.com/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf), the Kaiming Initialisation is used to tackle the problem of Vanishing or Exploding Gradient problems while using the [ReLU activation](https://www.dremio.com/wiki/relu-activation-function/) function. This problem occurs specifically when using Xavier Initialisation with the ReLU activation function, and hence the Kaiming Initialisation was introduced to tackle said problem by considering a random number using Gaussian Distribution (G) with mean 0 and standard deviation $ \\sqrt{\\frac{2}{n}} $.   \n",
    "\n",
    "$$ W \\sim U \\left[ -\\sqrt{\\frac{6}{inputs + outputs}} ,  \\sqrt{\\frac{6}{inputs + outputs}}\\right] $$ and   \n",
    "$$ W \\sim N \\left( 0 , \\sqrt{\\frac{2}{inputs}} \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ac7304-35e4-4fff-b362-71bc8d66911e",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3954b60-7ebb-4a6e-b8d6-94f316de2f34",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9424ce-ceb0-4984-89cb-cd6d83046d0f",
   "metadata": {},
   "source": [
    "This short description of weight initialisation gives an idea as to why proper procedure must be followed to initialise weights while model creation and what type of initialisation should be used when. The obvious steps which can be followed further is the investigation of convergence speed on training data. The investigation of tanh activation function consists of the usage of Xavier Initialisation corresponding to the ever so relevant Vanishing Gradient problem. It should also be observed that while weight initialisations help, they do not completely mitigate the presence of the vanishing or exploding gradient problem and further analysis and methods should be followed to fine tune the prediction process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4600e87-a8df-4996-9f28-ed31669c1379",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99808511-152f-48c9-8d63-79419a6bda7b",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bcebd6-cf52-487d-81e9-56897ada8332",
   "metadata": {},
   "source": [
    "[1] https://www.geeksforgeeks.org/vanishing-and-exploding-gradients-problems-in-deep-learning/   \n",
    "\n",
    "[2] https://www.baeldung.com/cs/ml-neural-network-weights  \n",
    "\n",
    "[3] http://proceedings.mlr.press/v9/glorot10a.html   \n",
    "\n",
    "[4] https://365datascience.com/tutorials/machine-learning-tutorials/what-is-xavier-initialization/#h_24242636975541686829817569   \n",
    "\n",
    "[5] https://openaccess.thecvf.com/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f046e1-adbb-406b-8af8-05e63aae3e78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
